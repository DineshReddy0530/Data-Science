{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con=sqlite3.connect('database.sqlite')\n",
    "filtered_data=pd.read_sql_query(\"\"\"select * from reviews where Score!=3\"\"\",con)\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(525814, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def  partition(x):\n",
    "    if x<3:\n",
    "        return 'negative'\n",
    "    return 'positive'\n",
    "\n",
    "actualScore=filtered_data['Score']\n",
    "positiveNeg=actualScore.map(partition)\n",
    "filtered_data['Score']=positiveNeg\n",
    "filtered_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display=pd.read_sql_query(\"\"\"select * from reviews where Score!=0 AND UserId='AR5J8UI46CURR' order by ProductId\"\"\",con)\n",
    "sorted_data=filtered_data.sort_values(by='ProductId',axis=0,ascending=True,inplace=False)\n",
    "final=sorted_data.drop_duplicates(subset={'UserId','Time','Text','ProfileName'},keep='first',inplace=False)\n",
    "final.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "display=pd.read_sql_query(\"\"\"select * from reviews where Score!=3 AND Id=44737 OR Id=64422\"\"\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    307061\n",
       "negative     57110\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DINESH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cacti\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"Cacti\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delicious\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('delicious'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect=CountVectorizer()\n",
    "final_counts=count_vect.fit_transform(final['Text'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364171, 115281)\n",
      "(364171, 11)\n"
     ]
    }
   ],
   "source": [
    "print(final_counts.shape)\n",
    "print(final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hasn', \"won't\", \"shan't\", 'me', 'so', 'be', 'but', 'how', 'are', 'ma', 'our', 'on', \"mustn't\", \"hasn't\", 'very', \"should've\", 'a', 'before', 'now', 'isn', 'those', 'yourself', \"it's\", 'did', 'during', 'will', 'when', 'each', 'wouldn', 'other', 'll', 'who', 'no', 'mustn', \"needn't\", 'under', 'itself', 'them', \"you'll\", 'her', 'over', 'doing', 'we', 'your', 'she', \"you'd\", \"didn't\", 'is', 'below', 'until', 's', 'had', 'own', 'at', \"weren't\", 'd', 'my', 'they', 'hadn', \"wouldn't\", 'couldn', 'hers', 'you', 'weren', 'herself', 'having', 'more', 'doesn', 'aren', \"shouldn't\", 'y', 'mightn', 'shouldn', 'whom', 'above', 'do', 'their', 'after', 'ours', 'themselves', 'between', 'i', 'didn', 'its', 'does', 'off', 'wasn', 'most', \"wasn't\", 'which', 'by', 'again', \"haven't\", \"hadn't\", 'then', 'for', 'o', 'just', 'all', 'ain', 'if', 'were', 'what', 'there', 'an', 'it', 'too', 'with', 'of', 'nor', \"you're\", 'this', 'won', 'theirs', 'needn', 'why', 'same', 'through', 'into', \"isn't\", 've', 'yourselves', 'and', 'being', \"mightn't\", 'few', 'ourselves', 'myself', 'shan', 'further', 'to', \"that'll\", 'been', 're', 'out', 'm', 'because', 'has', 'both', \"don't\", 'that', 'any', \"you've\", 'himself', \"she's\", 'from', 'he', 'should', 'not', 'such', 'while', 'only', 'where', 'don', \"doesn't\", 'his', 'was', 'here', \"couldn't\", 'against', 'in', 't', 'can', 'the', 'up', 'these', 'than', 'once', 'down', 'about', 'haven', 'some', 'yours', 'as', 'or', 'am', 'have', 'him', \"aren't\"}\n",
      "****************\n",
      "tasti\n"
     ]
    }
   ],
   "source": [
    "stop=set(stopwords.words('english'))\n",
    "sno=nltk.stem.SnowballStemmer('english')\n",
    "def cleanhtml(sentence):\n",
    "    cleanr=re.compile('<.*?>')\n",
    "    cleantext=re.sub(cleanr,\" \",sentence)\n",
    "    return cleantext\n",
    "\n",
    "def cleanpunctuation(sentence):\n",
    "    cleaned=re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned=re.sub(r'[.|,|(|)|\\|/]',r' ',cleaned)\n",
    "    return cleaned\n",
    "\n",
    "print(stop)\n",
    "print('****************')\n",
    "print(sno.stem('tasty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DINESH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for imlementing step- by-step preprocessing\n",
    "\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello,', 'dinesh;', 'how', 'r', 'u']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hello, dinesh; how r u\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dinesh']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'dinesh'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "strl=' '\n",
    "final_string=[]\n",
    "all_positive_words=[]\n",
    "all_negative_words=[]\n",
    "a=''\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cleanhtml(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunctuation(w).split():\n",
    "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):\n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if(final['Score'].values)[i]=='positive':\n",
    "                        all_positive_words.append(s)\n",
    "                    if(final['Score'].values)[i]=='negative':\n",
    "                        all_negative_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "    strl=b\" \".join(filtered_sentence)    \n",
    "    \n",
    "    \n",
    "    final_string.append(strl)\n",
    "    i+=1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['CleanedText']=final_string\n",
    "final.head(3)\n",
    "\n",
    "\n",
    "conn=sqlite3.connect('final.sqlite')\n",
    "c=conn.cursor()\n",
    "conn.text_factory=str\n",
    "final.to_sql('Reviews',conn,schema=None,if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Positive Words  [(b'like', 139429), (b'tast', 129047), (b'good', 112766), (b'flavor', 109624), (b'love', 107357), (b'use', 103888), (b'great', 103870), (b'one', 96726), (b'product', 91033), (b'tri', 86791), (b'tea', 83888), (b'coffe', 78814), (b'make', 75107), (b'get', 72125), (b'food', 64802), (b'would', 55568), (b'time', 55264), (b'buy', 54198), (b'realli', 52715), (b'eat', 52004)]\n",
      "Most Common Negative Words  [(b'tast', 34585), (b'like', 32330), (b'product', 28218), (b'one', 20569), (b'flavor', 19575), (b'would', 17972), (b'tri', 17753), (b'use', 15302), (b'good', 15041), (b'coffe', 14716), (b'get', 13786), (b'buy', 13752), (b'order', 12871), (b'food', 12754), (b'dont', 11877), (b'tea', 11665), (b'even', 11085), (b'box', 10844), (b'amazon', 10073), (b'make', 9840)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist_positive=nltk.FreqDist(all_positive_words)\n",
    "freq_dist_negative=nltk.FreqDist(all_negative_words)\n",
    "print(\"Most Common Positive Words \",freq_dist_positive.most_common(20))\n",
    "print(\"Most Common Negative Words \",freq_dist_negative.most_common(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect=CountVectorizer(ngram_range=(1,2))\n",
    "final_bigram_counts=count_vect.fit_transform(final['Text'].values)\n",
    "final_bigram_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vect=TfidfVectorizer(ngram_range=(1,2))\n",
    "final_tf_idf=tf_idf_vect.fit_transform(final['Text'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 2910192)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2910192"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=tf_idf_vect.get_feature_names()\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ales until',\n",
       " 'ales ve',\n",
       " 'ales would',\n",
       " 'ales you',\n",
       " 'alessandra',\n",
       " 'alessandra ambrosia',\n",
       " 'alessi',\n",
       " 'alessi added',\n",
       " 'alessi also',\n",
       " 'alessi and']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[100000:100010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(final_tf_idf[3,:].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "\n",
    "top_tfidf=top_tfidf_feats(final_tf_idf[1,:].toarray()[0],features,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sendak books</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rosie movie</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paperbacks seem</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cover version</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>these sendak</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the paperbacks</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pages open</td>\n",
       "      <td>0.173437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>really rosie</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>incorporates them</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>paperbacks</td>\n",
       "      <td>0.168074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>however miss</td>\n",
       "      <td>0.164269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hard cover</td>\n",
       "      <td>0.164269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>seem kind</td>\n",
       "      <td>0.161317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>up reading</td>\n",
       "      <td>0.156867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>that incorporates</td>\n",
       "      <td>0.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the pages</td>\n",
       "      <td>0.149737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sendak</td>\n",
       "      <td>0.149737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rosie</td>\n",
       "      <td>0.146786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>of flimsy</td>\n",
       "      <td>0.146786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>two hands</td>\n",
       "      <td>0.145130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>movie that</td>\n",
       "      <td>0.144374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>reading these</td>\n",
       "      <td>0.137184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>too do</td>\n",
       "      <td>0.134491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>incorporates</td>\n",
       "      <td>0.134147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>flimsy and</td>\n",
       "      <td>0.132254</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature     tfidf\n",
       "0        sendak books  0.173437\n",
       "1         rosie movie  0.173437\n",
       "2     paperbacks seem  0.173437\n",
       "3       cover version  0.173437\n",
       "4        these sendak  0.173437\n",
       "5      the paperbacks  0.173437\n",
       "6          pages open  0.173437\n",
       "7        really rosie  0.168074\n",
       "8   incorporates them  0.168074\n",
       "9          paperbacks  0.168074\n",
       "10       however miss  0.164269\n",
       "11         hard cover  0.164269\n",
       "12          seem kind  0.161317\n",
       "13         up reading  0.156867\n",
       "14  that incorporates  0.155100\n",
       "15          the pages  0.149737\n",
       "16             sendak  0.149737\n",
       "17              rosie  0.146786\n",
       "18          of flimsy  0.146786\n",
       "19          two hands  0.145130\n",
       "20         movie that  0.144374\n",
       "21      reading these  0.137184\n",
       "22             too do  0.134491\n",
       "23       incorporates  0.134147\n",
       "24         flimsy and  0.132254"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138706    b'witti littl book make son laugh loud recit c...\n",
       "138688    b'grew read sendak book watch realli rosi movi...\n",
       "138689    b'fun way children learn month year learn poem...\n",
       "138690    b'great littl book read nice rhythm well good ...\n",
       "138691    b'book poetri month year goe month cute littl ...\n",
       "138693    b'charm rhyme book describ circumst eat dont c...\n",
       "138694    b'set asid least hour day read son point consi...\n",
       "138695    b'rememb book childhood got kid good rememb ki...\n",
       "138696    b'great book ador illustr true classic kid lov...\n",
       "138697    b'book famili favorit read children small orde...\n",
       "138687    b'get movi sound track sing along carol king g...\n",
       "138698    b'author wrote wild thing carol king wrote gre...\n",
       "138700    b'great book perfect condit arriv short amount...\n",
       "138701    b'ive alway love chicken soup rice late ethel ...\n",
       "138702    b'book purchas birthday gift year old boy sque...\n",
       "138703    b'year old daughter brought book home school l...\n",
       "138704    b'book contain collect twelv short statement e...\n",
       "138705    b'young boy describ use chicken soup rice mont...\n",
       "138707    b'daughter love realli rosi book introduc real...\n",
       "138708    b'one best children book ever written mini ver...\n",
       "138709    b'june saw charm group rose begin droop pep ch...\n",
       "138699    b'classic children book cant wrong read kid or...\n",
       "138686    b'entertain rhyme catchi illustr imagin fit ri...\n",
       "138692    b'chicken soup rice mauric sendak josh grossma...\n",
       "138680    b'wonder littl book love year ago twin love en...\n",
       "138677    b'children love book first grader got christma...\n",
       "138678    b'one earliest memori book mother read constan...\n",
       "138685    b'grand daughter favorit book read love rhythm...\n",
       "138684    b'copi smaller expect most didnt pay attent li...\n",
       "138679    b'give five star mauric sendak stori one star ...\n",
       "                                ...                        \n",
       "35419     b'trader joe product good qualiti buy straight...\n",
       "195185    b'gave bulb receptionist local elder home volu...\n",
       "494393    b'tri read ingredi includ sweet agav extract p...\n",
       "134853    b'wateri tasteless pinot noir planet cali tool...\n",
       "430102    b'usual drink green machin realli want tri blu...\n",
       "264734    b'purchas sever amarylli seller none compar th...\n",
       "264735    b'neighborhood recent decid creat neighborhood...\n",
       "241028    b'true love gave christma bloom amarylli colle...\n",
       "269822    b'bit hesit buy tea due low price truth high q...\n",
       "343072    b'avid tea drinker enjoy tea warm cold depend ...\n",
       "427660    b'wife use unsweeten ice tea year great your w...\n",
       "184728    b'product person pantri delici love kid ask pa...\n",
       "178140    b'use product month never back sweeten tast am...\n",
       "178139    b'great product expens easi use ship quick eas...\n",
       "178138    b'zero sweetner best ever use one drop cup cof...\n",
       "178135    b'fat skinni zero wonder sweeten use everyth b...\n",
       "178136    b'use fat skinni zero sweeten almost year swee...\n",
       "178134    b'use product bit six month absolut love use a...\n",
       "178137    b'sweetner great littl bit goe long way use se...\n",
       "178142    b'ive use fat skinni zero sinc first introduc ...\n",
       "178141    b'use bake instead sugar use cake whip cream a...\n",
       "178143    b'fts zero best sweeten ever tri aftertast sto...\n",
       "178147    b'wonder product perfect use bake anyth need a...\n",
       "178146    b'packet powder sweeten usual littl dextros ak...\n",
       "178144    b'love sweeten use replac sugar everyth funni ...\n",
       "178145    b'love love sweeten use bake unsweeten flavor ...\n",
       "173675    b'tri sauc believ start littl sweet honey tast...\n",
       "204727    b'bought hazelnut past nocciola spread local s...\n",
       "5259      b'purchas product local store kid love quick e...\n",
       "302474    b'purchas send son whos away colleg deliv righ...\n",
       "Name: CleanedText, Length: 364171, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[\"CleanedText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ['Python is a scripting language','Perl is as scripting language','Java is a programming language']\n",
    "k_vect = TfidfVectorizer()\n",
    "z = k_vect.fit_transform(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8)\n"
     ]
    }
   ],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.39148397, 0.        , 0.39148397, 0.        ,\n",
       "        0.        , 0.66283998, 0.50410689],\n",
       "       [0.55249005, 0.32630952, 0.        , 0.32630952, 0.55249005,\n",
       "        0.        , 0.        , 0.42018292],\n",
       "       [0.        , 0.35959372, 0.6088451 , 0.35959372, 0.        ,\n",
       "        0.6088451 , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as', 'is', 'java', 'language', 'perl', 'programming', 'python', 'scripting']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n",
      "[[0.         0.42544054 0.         0.         0.72033345 0.54783215]\n",
      " [0.         0.42544054 0.72033345 0.         0.         0.54783215]\n",
      " [0.65249088 0.38537163 0.         0.65249088 0.         0.        ]]\n",
      "['java', 'language', 'perl', 'programming', 'python', 'scripting']\n"
     ]
    }
   ],
   "source": [
    "string = ['Python is a scripting language','Perl is as scripting language','Java is a programming language']\n",
    "k_vect = TfidfVectorizer(stop_words='english')\n",
    "z = k_vect.fit_transform(string)\n",
    "print(z.shape)\n",
    "print(z.toarray())\n",
    "print(k_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n",
      "[[0 1 0 0 1 1]\n",
      " [0 1 1 0 0 1]\n",
      " [1 1 0 1 0 0]]\n",
      "['java', 'language', 'perl', 'programming', 'python', 'scripting']\n"
     ]
    }
   ],
   "source": [
    "string = ['Python is a scripting language','Perl is as scripting language','Java is a programming language']\n",
    "k_vect = CountVectorizer(stop_words='english')\n",
    "z = k_vect.fit_transform(string)\n",
    "print(z.shape)\n",
    "print(z.toarray())\n",
    "print(k_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8)\n",
      "[[0 1 0 1 0 0 1 1]\n",
      " [1 1 0 1 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0]]\n",
      "['as', 'is', 'java', 'language', 'perl', 'programming', 'python', 'scripting']\n"
     ]
    }
   ],
   "source": [
    "string = ['Python is a scripting language','Perl is as scripting language','Java is a programming language']\n",
    "k_vect = CountVectorizer()\n",
    "z = k_vect.fit_transform(string)\n",
    "print(z.shape)\n",
    "print(z.toarray())\n",
    "print(k_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle+\n",
    "\n",
    "\n",
    "model=KeyedVectors.load_word2vec_format('GoogleNews-vector-Negative300')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
